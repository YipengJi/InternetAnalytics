{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 2: Latent semantic indexing\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *J*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Dennis Gankin*\n",
    "* *Name 2*\n",
    "* *Name 3*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 2 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from utils import load_pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4: Latent semantic indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "TF_m = load_pkl('tfidf.pkl')\n",
    "terms = load_pkl('terms.pkl')\n",
    "courses = load_pkl('courses.pkl')\n",
    "\n",
    "#n terms, m documents\n",
    "n, m = TF_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "U, S, V_t = svds(TF_m, k=300, which='LM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO Describe the rows and columns of U and V , and the values of S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U$: Term-concept mapping\n",
    "\n",
    "The $n$ rows of the $U$-matrix given by the SVD, give a mapping from term to concept. Each row is the mapping for one term, and each value $v_i$ in that row shows how strongly that term relates to concept $c_i$\n",
    "\n",
    " $V^T$: Course-concept mapping\n",
    "\n",
    "Similarly, the $m$ columns of the $V^T$ matrix shows how strongly each course corresponds to each concept.\n",
    "\n",
    " $S$: Concept-\"strength\"\n",
    "\n",
    "The singular values of $S$ shows how \"strong\" the concept is - the bigger the value is, the \"stronger\" the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 344.31354526,  223.54598261,  211.23490744,  204.18159738,\n",
       "        192.59066714,  190.12085322,  187.87139273,  186.11051083,\n",
       "        182.05487293,  177.11191671,  171.67441762,  170.92157508,\n",
       "        168.79107365,  163.88075299,  160.85639893,  160.70081938,\n",
       "        158.30216791,  156.76891911,  154.34270592,  152.88152916])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singular20 = S[::-1][:20]\n",
    "singular20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[118551.81745109936,\n",
       " 49972.806340782823,\n",
       " 44620.186123127998,\n",
       " 41690.124708290095,\n",
       " 37091.165068619892,\n",
       " 36145.938828648803,\n",
       " 35295.660205228443,\n",
       " 34637.122241329547,\n",
       " 33143.976759353827,\n",
       " 31368.631040412936,\n",
       " 29472.105666437194,\n",
       " 29214.184826477391,\n",
       " 28490.426545137649,\n",
       " 26856.901200287168,\n",
       " 25874.781076314903,\n",
       " 25824.753349977462,\n",
       " 25059.576364671633,\n",
       " 24576.493999927687,\n",
       " 23821.670870634709,\n",
       " 23372.761958302566]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenv20= [x*x for x in singular20]\n",
    "eigenv20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5: Topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 :\n",
      "['dwarf', 'observationshydrogen', 'holesstellar', 'meynet', 'simulationsstar', 'contractu', 'apparatus', 'magnetohydrodynam', 'microinst', 'aspectsstructurefinanci', 'aspectsregulatori', 'aspectsmarket', 'aspectslega', 'aspectsglp', 'edmt']\n",
      "Topic 2 :\n",
      "['magnet', 'micro', 'cell', 'mem', 'photon', 'sensor', 'print', 'materi', 'imag', 'light', 'devic', 'microscopi', 'laser', 'electron', 'optic']\n",
      "Topic 3 :\n",
      "['research', 'develop', 'semest', 'solid', 'form', 'treatment', 'week', 'recycl', 'report', 'excurs', 'urban', 'studio', 'project', 'architectur', 'wast']\n",
      "Topic 4 :\n",
      "['system', 'citi', 'comput', 'lab', 'signal', 'imag', 'algorithm', 'speech', 'robot', 'project', 'digit', 'urban', 'data', 'studio', 'architectur']\n",
      "Topic 5 :\n",
      "['mass', 'fluid', 'numer', 'structur', 'thermodynam', 'seismic', 'ah', 'heat', 'protein', 'energi', 'chemic', 'reaction', 'flow', 'cell', 'steel']\n",
      "Topic 6 :\n",
      "['immers', 'scientif', 'wet', 'cell', 'lab', 'obtain', 'sv', 'laboratori', 'ssv', 'host', 'biolog', 'laser', 'protein', 'report', 'optic']\n",
      "Topic 7 :\n",
      "['filter', 'model', 'statist', 'voic', 'numer', 'linear', 'algorithm', 'robot', 'code', 'signal', 'recognit', 'seismic', 'wast', 'steel', 'speech']\n",
      "Topic 8 :\n",
      "['algorithm', 'numer', 'ah', 'code', 'voic', 'chemic', 'flow', 'model', 'wast', 'cell', 'protein', 'signal', 'process', 'recognit', 'speech']\n",
      "Topic 9 :\n",
      "['obtain', 'technolog', 'ssv', 'mem', 'laboratori', 'electron', 'host', 'wearabl', 'circuit', 'report', 'lab', 'sensor', 'devic', 'print', 'robot']\n",
      "Topic 10 :\n",
      "['stoke', 'comput', 'simul', 'finit', 'optic', 'transfer', 'turbul', 'laser', 'equat', 'fluid', 'heat', 'numer', 'snow', 'flow', 'ah']\n"
     ]
    }
   ],
   "source": [
    "s = np.diag(S)\n",
    "# combination of terms\n",
    "for topic in range(-1,-11,-1):\n",
    "    words = [terms[t] for t in np.argsort(U[:,topic])[-15:]]\n",
    "    print('Topic', -topic,':')\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 :\n",
      "['contact' 'edmt' 'administration' 'enrollment' 'training' 'rotation']\n",
      "Topic 2 :\n",
      "['addresses' 'implementation' 'organic' 'printed' 'electronics'\n",
      " 'technologies' 'large' 'area' 'manufacturing' 'techniques']\n",
      "Topic 3 :\n",
      "['studio' 'explores' 'meaningful' 'form' 'generating' 'processes'\n",
      " 'algorithmic' 'parametric' 'tools' 'introduces']\n",
      "Topic 4 :\n",
      "['studio' 'explores' 'meaningful' 'form' 'generating' 'processes'\n",
      " 'algorithmic' 'parametric' 'tools' 'introduces']\n",
      "Topic 5 :\n",
      "['covers' 'basic' 'aspects' 'numerical' 'discretization' 'solution' 'fluid'\n",
      " 'flow' 'heat' 'transfer']\n",
      "Topic 6 :\n",
      "['engage' 'laboratory' 'based' 'project' 'field' 'molecular' 'medicine'\n",
      " 'neuroscience' 'bioengineering' 'projects']\n",
      "Topic 7 :\n",
      "['advanced' 'topics' 'structural' 'steel' 'seismic' 'topics' 'include'\n",
      " 'bolted' 'welded' 'beam']\n",
      "Topic 8 :\n",
      "['goal' 'provide' 'main' 'formalisms' 'models' 'algorithms' 'required'\n",
      " 'implementation' 'advanced' 'speech']\n",
      "Topic 9 :\n",
      "['engage' 'laboratory' 'based' 'project' 'field' 'molecular' 'medicine'\n",
      " 'neuroscience' 'bioengineering' 'projects']\n",
      "Topic 10 :\n",
      "['covers' 'principles' 'snow' 'physics' 'snow' 'hydrology' 'snow'\n",
      " 'atmosphere' 'interaction' 'snow']\n"
     ]
    }
   ],
   "source": [
    "# combination of documents\n",
    "for topic in range(-1,-11,-1):\n",
    "    # Collect some words from each course\n",
    "    words = np.concatenate([courses[t]['cut_wordlist'] for t in np.argsort(V_t[topic])[-2:]])\n",
    "    print('Topic', -topic,':')\n",
    "    print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: find lables, is the thing above combionation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.6: Document similarity search in concept-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_document_similarity(query):\n",
    "    similarities = np.zeros(len(courses))\n",
    "    for term in query.split(' '):\n",
    "        t_index = terms.index(term)\n",
    "        u_t = U[t_index]\n",
    "        \n",
    "        for ix, d in enumerate(courses):\n",
    "            v_T_d = V_t[:,ix]\n",
    "    \n",
    "            s_v_T_d = np.dot(s, v_T_d)\n",
    "            nominator = np.dot(u_t, s_v_T_d)\n",
    "            denominator = np.linalg.norm(u_t) * np.linalg.norm(s_v_T_d)\n",
    "            sim = nominator / denominator\n",
    "            similarities[ix] += sim\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def LSI_search(terms, no_top=5):\n",
    "    similiarities = term_document_similarity(terms)\n",
    "    top_results = np.argsort(similiarities)[::-1][0:no_top]\n",
    "    for top in top_results:\n",
    "        print('{0}: {1}'.format(courses[top]['name'], top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational Social Media: 798\n",
      "Social media: 407\n",
      "Studio MA2 (Escher et GuneWardena): 59\n",
      "Human computer interaction: 521\n",
      "Transport phenomena II: 29\n"
     ]
    }
   ],
   "source": [
    "LSI_search('facebook', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied stochastic processes: 80\n",
      "Applied probability & stochastic processes: 398\n",
      "Markov chains and algorithmic applications: 245\n",
      "Supply chain management: 44\n",
      "Mathematical models in supply chain management: 99\n"
     ]
    }
   ],
   "source": [
    "LSI_search('markov chain', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare with previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.7: Document-document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO equation?\n",
    "# best way for document-document similarity is computing cosine similarity between a given document, represented as topic vector V_t and rest of the document-topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(d1, d2):\n",
    "    return np.dot(d1, d2) / (np.linalg.norm(d1) * np.linalg.norm(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A Network Tour of Data Science': 0.36356197773259208,\n",
       " 'Distributed information systems': 0.53311533332506555,\n",
       " 'Financial big data': 0.43771257172080819,\n",
       " 'Graph theory': 0.25801553622868179,\n",
       " 'Networks out of control': 0.34966640343643635}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IX_id = next(index for (index, d) in enumerate(courses) if d[\"courseId\"] == \"COM-308\")\n",
    "\n",
    "IX_sim = np.apply_along_axis(cosine_sim, 0, V_t, V_t[:, IX_id])\n",
    "IX_sim5 = np.argsort(IX_sim)[::-1][0:6]\n",
    "\n",
    "IX_top_courses = {}\n",
    "for i, course_id in enumerate(IX_sim5):\n",
    "    if course_id != IX_id:\n",
    "        IX_top_courses[courses[course_id]['name']] = np.sort(IX_sim)[::-1][i]\n",
    "\n",
    "IX_top_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO short comment?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
